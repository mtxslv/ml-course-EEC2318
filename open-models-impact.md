[Large Language Models (LLMs) are out in the wild](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). These open foundation models stir up community interest and fear around how they might be used, for both good and bad ends. Given such context, this short essay briefly discusses the positive and negative aspects of sharing NLP models, based on [Kapoor et al's _On the Societal Impact of Open Source Foundation Models_](https://crfm.stanford.edu/open-fms/paper.pdf). At last, I introduce my analysis of this issue.

Let's start by recalling the good results of open models. First of all, since ML models increasingly influence society, closed foundation models' behavior is unilaterally determined by developers and stakeholders. Open foundation models allow for greater diversity in defining which behavior is acceptable. Moreover, broader access to open models enables inclusion in science and increasing innovation. From the societal standpoint, the open weights make up for some transparency and accountability. At last, more diverse downstream model behaviors reduce the danger of market concentration that yields to systemic risk due to widespread dependence on a single (or few) technology(ies).

But life is not a bed of roses. Sharing models brings up their issues as well. For an example of misuse, the human-like text generation capability of these systems can be used as a tool for spear-phishing attacks ([Hazell, 2023](https://arxiv.org/pdf/2305.06972v1.pdf)). Ethics is also a concern. Once Generative AI can produce high-quality written and visual content, how do we assign responsibility for these outputs ([Mann, 2024](https://www.nature.com/articles/s42256-023-00653-1))?

I believe the gains of having open-source models outweigh the risks. And this happens for a simple reason: all these concerns would be badly shaped had we not seen what they can do from so many perspectives. Moreover,  we should pursue this initiative because only by sharing it with society we will find out how to address the misuse.