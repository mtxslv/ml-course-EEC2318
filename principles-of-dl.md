# Principle Concepts

Current AI systems contain billions of elementary components, or computational blocks, loosely based on biological neurons. These are organized into sequential computational layers (called deep neural nets, or DNN) that, after some initialization and training, _learn_ a refined representation capable of transforming data into something useful for a given task.





# Implications


# My Ideas

- There is something that bothers me a lot regarding AI. It feels like 'mathematics applied'. That's y I decided to learn it in the first place. Instead, when I see people '''using''' it on practice, they just run some tutorial on a subset of benchmark data and that's it. Like... wth? Where is the learning? Where is the science of the data? It is frustating because it has no art, no technique. It is even more dump than just writing a script to do the very same task. When reading the chapter's beginning I can only feel hopeless... so at the end there is no learning? Damn bro...

---

# On the pursue of a universal (?) theory...


The ML results are impressive. Curiously, its theory is still new and immature, disconnected from the practice. That is why [Roberts and Yaida decided to write The Principles of Deep Learning Theory](https://arxiv.org/pdf/2106.10165.pdf), aiming to analyze DNN in a relevant way. But how could them do it? We have elementary components changing and reacting at the same time: a baffling situation that _apparently_ hinders any chance of understanding why a given function is learned instead of any other.

Luckly for us, **theoretical physics** is used to find **simple, effective theories** for multi-component complicated systems. Take thermodynamics... in the 18th century some English gentlemen came up with the steam engine. In order to explain the empirical observations on the mechanics of steam, it took an insane amount of effort from guys like Maxwell, Boltzmann and Gibbs to link the experiments to the principles (statistical mechanics).

This works because physical theories aren't concerned about phenomena are natural or artificial. It only connects observables to the fundamental parematers. 
More importantly, we should recall we already know the intricacies of neural nets because each component (neuron) is explicitly placed (or defined) by us. We then know the mechanism transforming inputs in outputs through a cascade of signals. We just need to  grasp the macroscopic behaviour.