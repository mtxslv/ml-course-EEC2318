# Welcome! ðŸ‘‹ðŸ‘‹

This repo contains the materials associated with the PPgEEEC's Machine Learning course (EEC2318). Official material available on [ivanovitchm's PPGEEC2318 repo](https://github.com/ivanovitchm/PPGEEC2318).

# A brief reflection on ML Systems ðŸ¤–âœ’ï¸

In November 2022, OpenAI released ChatGPT. [Fast foward to June 2023, and it occupies the first two most searched queries worldwide, according to Google trends](https://trends.google.com/trends/explore?date=2022-11-29%202023-07-01&hl=en), experiencing [a sharp increase in search around that time](https://trends.google.com/trends/explore?date=2022-11-29%202024-03-18&q=%2Fg%2F11khcfz0y2&hl=en-US). Moreover, at the end of that year its annual recurring revenue crossed $2B, a 900% increase comparing to an estimated $200M at the end of 2022 ([Sacra](https://sacra.com/c/openai/)). These great figures are followed by an equally great spend to keep the system working: [ChatGPT could cost over $700000 per day to operate](https://www.businessinsider.com/how-much-chatgpt-costs-openai-to-run-estimate-report-2023-4#post-headline), with inference costs being worst than training costs, [say Dylan Patel and Afzal Ahmad in SemiAnalysis](https://www.semianalysis.com/p/the-inference-cost-of-search-disruption). This hints something interesting: **ML systems are not just about ML algorithms**. They are also about the entire ML system, including not only the infrastructure and the training, but also the data, the business requirements, and the logic for monitoring, and updating your models (besides, of course, developing). In few words: the needs of an AI system in research and in production are different.

[OpenAI has shown the world that ML can be a nifty tool for a lot of problems](https://hbr.org/2022/12/chatgpt-is-a-tipping-point-for-ai). However, it is not a magic that can solve all of them. Even for problems that ML can solve, ML solutions might not be the optimal solutions. Before starting an ML project, you might want to ask whether ML is necessary or cost-effective. As a quick reminder, [OpenAI has a funding of $11.3bi and valuation of $80bi](https://tracxn.com/d/companies/openai/__kElhSG7uVGeFk1i71Co9-nwFtmtyMVT7f-YHMn4TFBg), while [Facebook had a funding around $1.3bi (excluding loans) and was evaluated at $50bi](https://fortune.com/2011/01/11/timeline-where-facebook-got-its-funding/). It shows how much money does a large AI company requires to achieve a given valuation when compared to a non-AI one (at that time Facebook was not interested on AI). A last important point is that ML algorithms may cause a whole set of troubles: [bias](https://www.forbes.com/sites/forbesbusinesscouncil/2023/08/14/can-ai-be-as-unethical-and-biased-as-humans/?sh=2dce8d0c6878) and [unethical use](https://www.vox.com/2020/6/8/21284005/urgent-threat-deepfakes-politics-porn-kristen-bell), [data privacy](https://www.reuters.com/legal/legalindustry/privacy-paradox-with-ai-2023-10-31/), among many others. 

Now we got the feeling we can't just think about ML as an isolated element: we need a holistic view (from business to technicalities). This view has a name and it is called [MLOps](https://ml-ops.org/), ensuring that all end-to-end components work together. Another detail to pay attention is the importance of tying ML performance to business performance, since managers' main goal is to [maximize profit](https://www.nytimes.com/1970/09/13/archives/a-friedman-doctrine-the-social-responsibility-of-business-is-to.html). As important as this is to be realistic about the expected returns. ML does not happen overnight, and its ROI depends on the adoption maturity (pipeline efficiency).

We should also keep in mind a ML's system basic requirements:
- reliability: The system should continue to perform correctly at the desired level of performance even in the face of adversity (hardware or software faults, and even human error);
- scalability: an ML system can grow in complexity, traffic volume, model count, etc;
- maintainability: many people (w/ diff backgrounds) own different parts of the process. It is important to structure the workload in a way each contributor works uses their preferred tools and the whole process is easy to understand and maintain;
- adaptability: the system should be able to adapt to new data, new models, new business requirements, etc.

With all these aspects in mind we finally can investigate a business process to discover potential bottlenecks, framing one of them into a ML-based solution to alleviate it. At last, we will work on it. But remember: developing an ML system is iterative and cyclic (project scoping, data engineering, model development, deployment, monitoring and continual learning, and business analysis all happen over and over again).

Now I lil word on a fundamental element of ML services: data.

The rise of ML in recent years is tightly coupled with the rise of big data: complex on their own, streaming or historical. Each industry has their own tech stack, with particular data models (description of real world data) and data storage engines (databases). Data in production often goes through multiple processes and services, each one with specific paradigms, such that knowing how to collect, process, store, retrieve, and process an increasingly growing
amount of data is essential to people who want to bring ML systems to the world. These steps are closely related to another abstraction: ETL, or "extract, transform and load". Let's quickly share some ideas associated to the majority of these terms:
- collect: data comes from many different sources, with different characteristics, processing, and purposes. They may be inputted by an user (thus being completed malformatted), generated by a system (fast-growing), or provided by a third-party (often cleaned). Here lies the "Extract" portion of ETL;
- process: after cleaning, it is important to think how to represent the data in your databases, a concept called Data Model. Data Models affect the way we build systems, and each model makes it easier to do something opposite to something else. Popular data models are the Relational Data Model (SQL) and NoSQL. The ETL's "Transform" stage happens here;
- store (or persist): a not necessarily that straightforward probably costly process. It is important to think about how data will be used in the future to use a format that makes sense in the present. A term to be familiar with is _data serialization_. Common formats one is inevitably destined to find are: CSV or JSON (text), or Parquet (binary). You can liken 'store' to ETL's "Load";
- retrieve: different databases are optimized for different workloads. For instance, a transactional databases is geared towards logging actions (tweets, orders, uploading a file, etc), while an analytical database is used to answer questions associated with aggregations of data (min, max, average, etc).

Hopefully, it is clear now that data _flows_ from one process to another. There are some ways how this can happen:
- Data passes through databases, where processes write data into databases;
- Data passes through services, where services send the data directly through the network; 
- Data passes through real-time transport.
Now, regarding how how many times that happens we have two modes:
- Batch processing: (historical) data is periodically processed in batches;
- Streaming processing: data is processed in real-time (or in short-periods).

This discussion aimed to prompt a new understanding of how machine learning systems are wired and thought. Such view is important to give a new, grounded sense of how things work in the real-world. 

> This text is based on the first 3 chapters of Chip Huyen's [Designing Machine Learning Systems (O'Reilly 2022)](https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/) book.